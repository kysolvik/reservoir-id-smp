{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kysolvik/reservoir-id-smp/blob/main/reservoir-id-smp/train/train_smp_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "538971fa"
      },
      "outputs": [],
      "source": [
        " # Install required libs\n",
        "!pip uninstall crcmod\n",
        "!pip install --no-cache-dir -U crcmod\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install pytorch-lightning==1.9\n",
        "!pip install albumentations\n",
        "!pip install torch"
      ],
      "id": "538971fa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a68bb96"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "import albumentations as albu\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "import PIL\n",
        "import torch\n",
        "import numpy as np\n",
        "import segmentation_models_pytorch as smp\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "9a68bb96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoyquI6gAQG2"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "pl.seed_everything(78, workers=True)"
      ],
      "id": "zoyquI6gAQG2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlIj-Cdd34jv"
      },
      "outputs": [],
      "source": [
        "# Load mean and std for scaling\n",
        "!gsutil cp 'gs://res-id/cnn/training/example_prepped/mean_std_v1.npy' .\n",
        "mean_std = np.load('./mean_std_v1.npy')"
      ],
      "id": "HlIj-Cdd34jv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98a4681b"
      },
      "outputs": [],
      "source": [
        "# Download data\n",
        "!gsutil cp gs://res-id/cnn/training/example_prepped/reservoirs_10band.zip .\n",
        "!mkdir -p ./data\n",
        "!unzip gs://res-id/cnn/training/example_prepped/reservoirs_10band.zip -d ./data/"
      ],
      "id": "98a4681b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41fb1ef8"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = './data/reservoirs_10band/'\n",
        "\n",
        "\n",
        "x_train_dir = os.path.join(DATA_DIR, 'img_dir/train')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'ann_dir/train')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'img_dir/val')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'ann_dir/val')\n",
        "\n"
      ],
      "id": "41fb1ef8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4df71a5a"
      },
      "outputs": [],
      "source": [],
      "id": "4df71a5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "779c5608"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_training_augmentation():\n",
        "    \"\"\"Random combination of flipping, 90 deg rotation, and scaling in\"\"\"\n",
        "    train_transform = [\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "        albu.VerticalFlip(p=0.5),\n",
        "        albu.RandomRotate90(p=0.5),\n",
        "        albu.ShiftScaleRotate(scale_limit=(0, 0.25), rotate_limit=0, shift_limit=0.0, p=0.5, border_mode=0),\n",
        "    ]\n",
        "    return albu.Compose(train_transform, is_check_shapes=False)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    \"\"\"Helper function to rearrange dimensions to match Torch Tensor expectations\n",
        "\n",
        "    Before: xdim, ydim, bands\n",
        "    After: bands, xdim, ydim\n",
        "    \"\"\"\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def get_preprocessing():\n",
        "    \"\"\"Construct preprocessing transform\n",
        "\n",
        "    In this case, all it does is rearrange dims (see to_tensor above)\n",
        "    \"\"\"\n",
        "\n",
        "    _transform = [\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform, is_check_shapes=False)\n",
        "\n",
        "def normalize_image(ar, mean_std):\n",
        "    \"\"\"Scale to mean == 0, std == 1\"\"\"\n",
        "    return (ar - mean_std[0])/mean_std[1]\n"
      ],
      "id": "779c5608"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8f3f064"
      },
      "outputs": [],
      "source": [
        "class Dataset(BaseDataset):\n",
        "    \"\"\"Dataset Object Read images, apply augmentation and preprocessing transformations.\n",
        "\n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_values (list): values of classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline\n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing\n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    CLASSES = ['background', 'water']\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            images_dir,\n",
        "            masks_dir,\n",
        "            classes=None,\n",
        "            augmentation=None,\n",
        "            preprocessing=None,\n",
        "            mean_std = None\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id.replace('.tif', '.png')) for image_id in self.ids]\n",
        "\n",
        "        # convert str names to class values on masks\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        self.mean_std = mean_std\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        # read data\n",
        "        image = io.imread(self.images_fps[i])\n",
        "        mask = io.imread(self.masks_fps[i])\n",
        "\n",
        "        # extract certain classes from mask\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "\n",
        "        if self.mean_std is not None:\n",
        "            image = normalize_image(image, self.mean_std)\n",
        "\n",
        "\n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=(image.astype(np.float32)), mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            # if np.random.randint(0,1):\n",
        "            gauss_scale = np.random.uniform(0, 0.1)\n",
        "            image = image + np.random.normal(scale=gauss_scale, size=image.shape)\n",
        "\n",
        "\n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "\n",
        "        #Convert to PIL\n",
        "        return {'image':image, 'mask':mask}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "id": "d8f3f064"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6678098"
      },
      "outputs": [],
      "source": [
        "# helper function for data visualization\n",
        "def visualize(**images, bands_to_view=[0,1,2]):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        if i ==0:\n",
        "          plt.imshow(image[:,:, bands_to_view])\n",
        "          print('Final mean', image.mean())\n",
        "        else:\n",
        "          plt.imshow(image)\n",
        "\n",
        "    plt.show()"
      ],
      "id": "a6678098"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d79ed669"
      },
      "outputs": [],
      "source": [
        "# Lets look at data we have\n",
        "\n",
        "dataset = Dataset(x_train_dir, y_train_dir, classes=['Water'],\n",
        "                #   preprocessing=get_preprocessing(),\n",
        "                  augmentation=get_training_augmentation(),\n",
        "                  mean_std=mean_std,\n",
        ")\n",
        "\n",
        "batch = dataset[8] # get some sample\n",
        "visualize(\n",
        "    image=batch['image'],\n",
        "    water_mask=batch['mask'].squeeze(),\n",
        "    bands_to_view=[7,9,8]\n",
        ")"
      ],
      "id": "d79ed669"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdd6463b"
      },
      "outputs": [],
      "source": [
        "class ResModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, encoder_name, in_channels, out_classes, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = smp.Unet(encoder_name=encoder_name,\n",
        "                               in_channels=in_channels,\n",
        "                               classes=out_classes,\n",
        "                               aux_params=dict(classes=out_classes)\n",
        "        )\n",
        "\n",
        "        # DiceLoss generally does a good job for image segmentation\n",
        "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "        # Input images are 640x640, masks are 500x500. So we apply a crop to only look at the middle\n",
        "        self.crop_transform = torchvision.transforms.CenterCrop(500)\n",
        "\n",
        "    def forward(self, image):\n",
        "        # normalize image here\n",
        "        mask = self.model(image)[0]\n",
        "        return self.crop_transform(mask)\n",
        "\n",
        "    def shared_step(self, batch, stage):\n",
        "\n",
        "        image = batch[\"image\"]\n",
        "\n",
        "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
        "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
        "        assert image.ndim == 4\n",
        "\n",
        "        # Check that image dimensions are divisible by 32,\n",
        "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n",
        "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n",
        "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
        "        # and we will get an error trying to concat these features\n",
        "        h, w = image.shape[2:]\n",
        "        assert h % 32 == 0 and w % 32 == 0\n",
        "\n",
        "        mask = batch[\"mask\"]\n",
        "\n",
        "        # Shape of the mask should be [batch_size, num_classes, height, width]\n",
        "        # for binary segmentation num_classes = 1\n",
        "        assert mask.ndim == 4\n",
        "\n",
        "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
        "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
        "\n",
        "        logits_mask = self.forward(image)\n",
        "        # prob_mask = self.forward(image).sigmoid()\n",
        "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
        "        loss = self.loss_fn(logits_mask, mask)\n",
        "\n",
        "        # Lets compute metrics for some threshold\n",
        "        # first convert mask values to probabilities, then\n",
        "        # apply thresholding\n",
        "        prob_mask = logits_mask.sigmoid()\n",
        "        pred_mask = (prob_mask > 0.5).float()\n",
        "\n",
        "        # We will compute IoU metric by two ways\n",
        "        #   1. dataset-wise\n",
        "        #   2. image-wise\n",
        "        # but for now we just compute true positive, false positive, false negative and\n",
        "        # true negative 'pixels' for each image and class\n",
        "        # these values will be aggregated in the end of an epoch\n",
        "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"tp\": tp,\n",
        "            \"fp\": fp,\n",
        "            \"fn\": fn,\n",
        "            \"tn\": tn,\n",
        "        }\n",
        "\n",
        "    def shared_epoch_end(self, outputs, stage):\n",
        "        # aggregate step metrics\n",
        "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
        "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
        "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
        "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
        "\n",
        "        # Calculate IoU\n",
        "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
        "\n",
        "        metrics = {\n",
        "            f\"{stage}_dataset_iou\": dataset_iou,\n",
        "        }\n",
        "\n",
        "        self.log_dict(metrics, prog_bar=True)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, \"train\")\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        return self.shared_epoch_end(outputs, \"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, \"valid\")\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "\n",
        "        return self.shared_epoch_end(outputs, \"valid\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, \"test\")\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        return self.shared_epoch_end(outputs, \"test\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Learning rate, weight decay, and learning rate scheduler (decreasers by x0.9 per epoch)\n",
        "        # These all may need to be tweaked in detail\n",
        "        optim = torch.optim.Adam(self.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "        return [optim], [torch.optim.lr_scheduler.ExponentialLR(optim, 0.9)]"
      ],
      "id": "bdd6463b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0eef332"
      },
      "outputs": [],
      "source": [
        "# Here we define the model. We use a resnet34 encoder \"backbone\", which is a pretty simple one that does a good job\n",
        "model = ResModel(encoder_name=\"resnet34\", in_channels=10, out_classes=1)"
      ],
      "id": "a0eef332"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MouqFb5O5hq4"
      },
      "outputs": [],
      "source": [
        "# Stop training when validation IoU doesn't improve for 5 epochs\n",
        "early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(monitor=\"valid_dataset_iou\", mode='max', min_delta=0.00, patience=5)\n",
        "# Save best validation dataset IoU model only\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=\"/content/\", save_top_k=1, monitor=\"valid_dataset_iou\", mode='max')\n",
        "# Keep track of training and validation IoU while training\n",
        "logger = pl.loggers.CSVLogger(\"/content/logs/\", name=\"training_metrics\")"
      ],
      "id": "MouqFb5O5hq4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00535da1"
      },
      "outputs": [],
      "source": [
        "CLASSES = ['Reservoir'] # Only one class in this case\n",
        "# Training dataset. Note that it gets augmentation\n",
        "train_dataset = Dataset(\n",
        "    x_train_dir,\n",
        "    y_train_dir,\n",
        "    preprocessing=get_preprocessing(),\n",
        "    augmentation=get_training_augmentation(),\n",
        "    classes=CLASSES,\n",
        "    mean_std=mean_std,\n",
        ")\n",
        "\n",
        "# Validation dataset, no augmentation\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir,\n",
        "    y_valid_dir,\n",
        "    preprocessing=get_preprocessing(),\n",
        "    classes=CLASSES,\n",
        "    mean_std=mean_std,\n",
        ")\n",
        "\n",
        "# DataLoaders that the trainer will use for loading datasets in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=2)"
      ],
      "id": "00535da1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6668842"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define trainer. In this case, have GPU\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    max_epochs=50,\n",
        "    callbacks=[early_stop_callback, checkpoint_callback],\n",
        "    deterministic=True,\n",
        "    logger=logger\n",
        ")\n",
        "\n",
        "# Fit\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=valid_loader,\n",
        ")"
      ],
      "id": "b6668842"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}