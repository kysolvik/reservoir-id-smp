{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cS6OMcDlJ8Oq",
      "metadata": {
        "id": "cS6OMcDlJ8Oq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538971fa",
      "metadata": {
        "id": "538971fa"
      },
      "outputs": [],
      "source": [
        " # Install required libs\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install pytorch-lightning\n",
        "!pip install albumentations\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4lL899jmxoTL",
      "metadata": {
        "id": "4lL899jmxoTL"
      },
      "outputs": [],
      "source": [
        "!pip uninstall crcmod\n",
        "!pip install --no-cache-dir -U crcmod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a68bb96",
      "metadata": {
        "id": "9a68bb96"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "import albumentations as albu\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "import PIL\n",
        "import torch\n",
        "import numpy as np\n",
        "import segmentation_models_pytorch as smp\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zoyquI6gAQG2",
      "metadata": {
        "id": "zoyquI6gAQG2"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(78, workers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HlIj-Cdd34jv",
      "metadata": {
        "id": "HlIj-Cdd34jv"
      },
      "outputs": [],
      "source": [
        "!gsutil cp 'gs://res-id/cnn/training/prepped_gaip_landsat/mean_std_ls8_v9.npy' .\n",
        "mean_std = np.load('mean_std_ls8_v9.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1XhHGJLK11t_",
      "metadata": {
        "id": "1XhHGJLK11t_"
      },
      "outputs": [],
      "source": [
        "mean_std.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98a4681b",
      "metadata": {
        "id": "98a4681b"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://res-id/cnn/training/prepped_gaip_landsat/landsat8_v9_sr.zip .\n",
        "!mkdir -p ./data\n",
        "!unzip landsat8_v9_sr.zip -d ./data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fb1ef8",
      "metadata": {
        "id": "41fb1ef8"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = './data/landsat8_v9_sr/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df71a5a",
      "metadata": {
        "id": "4df71a5a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "x_train_dir = os.path.join(DATA_DIR, 'img_dir/train')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'ann_dir/train')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'img_dir/val')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'ann_dir/val')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779c5608",
      "metadata": {
        "id": "779c5608"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "        albu.VerticalFlip(p=0.5),\n",
        "        albu.RandomRotate90(p=1.0),\n",
        "\n",
        "\n",
        "        # albu.ShiftScaleRotate(scale_limit=(0, 0.05), rotate_limit=0, shift_limit=0.0, p=0.5, border_mode=0),\n",
        "\n",
        "    ]\n",
        "    return albu.Compose(train_transform, is_check_shapes=False)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def get_preprocessing():\n",
        "    \"\"\"Construct preprocessing transform\n",
        "\n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function\n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _transform = [\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform, is_check_shapes=False)\n",
        "\n",
        "def normalize_image(ar, mean_std):\n",
        "    return (ar - mean_std[0])/mean_std[1]\n",
        "    return ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05u0Ipx0pIBp",
      "metadata": {
        "id": "05u0Ipx0pIBp"
      },
      "outputs": [],
      "source": [
        "# # Example problem with GaussNoise:\n",
        "# im = np.random.normal(size=(2, 2, 3))\n",
        "# aug = albu.Compose([albu.GaussNoise(var_limit=0.5, p=1.0, always_apply=True),])\n",
        "# print(im)\n",
        "# print(aug(image=im)['image'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f3f064",
      "metadata": {
        "id": "d8f3f064"
      },
      "outputs": [],
      "source": [
        "class Dataset(BaseDataset):\n",
        "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "\n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline\n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing\n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            images_dir,\n",
        "            masks_dir,\n",
        "            augmentation=None,\n",
        "            preprocessing=None,\n",
        "            mean_std = None\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
        "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
        "\n",
        "        # convert str names to class values on masks\n",
        "\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        self.mean_std = mean_std\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        # read data\n",
        "        image = io.imread(self.images_fps[i])\n",
        "        mask = io.imread(self.masks_fps[i])\n",
        "\n",
        "        if self.mean_std is not None:\n",
        "           image = normalize_image(image, self.mean_std)\n",
        "\n",
        "        # IMPORTANT: Remove first band\n",
        "        image = image[:,:,1:-4]\n",
        "        mask = io.imread(self.masks_fps[i])\n",
        "\n",
        "        mask = np.expand_dims(mask, -1).astype('float')\n",
        "        # mask = mask.astype('float')\n",
        "\n",
        "\n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=(image.astype(np.float32)), mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            # # if np.random.randint(0,1):\n",
        "            # gauss_scale = np.random.uniform(0, 0.1)\n",
        "            # image = image + np.random.normal(scale=gauss_scale, size=image.shape)\n",
        "\n",
        "\n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "\n",
        "        #Convert to PIL\n",
        "        return {'image':image, 'mask':mask}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6678098",
      "metadata": {
        "id": "a6678098"
      },
      "outputs": [],
      "source": [
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        if i ==0:\n",
        "          plt.imshow(image[:,:, [0,1,2]])\n",
        "          print('Final mean', image.mean())\n",
        "        else:\n",
        "          plt.imshow(image)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79ed669",
      "metadata": {
        "id": "d79ed669"
      },
      "outputs": [],
      "source": [
        "# Lets look at data we have\n",
        "\n",
        "dataset = Dataset(x_train_dir, y_train_dir,\n",
        "                #   preprocessing=get_preprocessing(),\n",
        "                  augmentation=get_training_augmentation(),\n",
        "                  mean_std=mean_std,\n",
        ")\n",
        "\n",
        "batch = dataset[65] # get some sample\n",
        "visualize(\n",
        "    image=batch['image'],\n",
        "    water_mask=batch['mask'].squeeze(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S-LCkXcu-gM4",
      "metadata": {
        "id": "S-LCkXcu-gM4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd6463b",
      "metadata": {
        "id": "bdd6463b"
      },
      "outputs": [],
      "source": [
        "class ResModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, encoder_name, in_channels, out_classes, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = smp.MAnet(encoder_name=encoder_name, in_channels=in_channels, classes=out_classes,# encoder_weights=None, decoder_use_batchnorm=True,\n",
        "                                      aux_params=dict(\n",
        "                                          classes=out_classes)\n",
        "        )\n",
        "\n",
        "        # for image segmentation dice loss\n",
        "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "        self.crop_transform = torchvision.transforms.CenterCrop(500)\n",
        "\n",
        "    def forward(self, image):\n",
        "        # normalize image here\n",
        "        mask = self.model(image)[0]\n",
        "        return self.crop_transform(mask)\n",
        "\n",
        "    def shared_step(self, batch, stage):\n",
        "\n",
        "        image = batch[\"image\"]\n",
        "\n",
        "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
        "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
        "        assert image.ndim == 4\n",
        "\n",
        "        # Check that image dimensions are divisible by 32,\n",
        "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n",
        "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n",
        "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
        "        # and we will get an error trying to concat these features\n",
        "        h, w = image.shape[2:]\n",
        "        assert h % 32 == 0 and w % 32 == 0\n",
        "\n",
        "        mask = batch[\"mask\"]\n",
        "\n",
        "        # Shape of the mask should be [batch_size, num_classes, height, width]\n",
        "        # for binary segmentation num_classes = 1\n",
        "        assert mask.ndim == 4\n",
        "\n",
        "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
        "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
        "\n",
        "        logits_mask = self.forward(image)\n",
        "        # prob_mask = self.forward(image).sigmoid()\n",
        "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
        "\n",
        "        loss = self.loss_fn(logits_mask, mask)\n",
        "\n",
        "        # Lets compute metrics for some threshold\n",
        "        # first convert mask values to probabilities, then\n",
        "        # apply thresholding\n",
        "        prob_mask = logits_mask.sigmoid()\n",
        "        pred_mask = (prob_mask > 0.5).float()\n",
        "\n",
        "        # We will compute IoU metric by two ways\n",
        "        #   1. dataset-wise\n",
        "        #   2. image-wise\n",
        "        # but for now we just compute true positive, false positive, false negative and\n",
        "        # true negative 'pixels' for each image and class\n",
        "        # these values will be aggregated in the end of an epoch\n",
        "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"tp\": tp,\n",
        "            \"fp\": fp,\n",
        "            \"fn\": fn,\n",
        "            \"tn\": tn,\n",
        "        }\n",
        "\n",
        "    def shared_epoch_end(self, outputs, stage):\n",
        "        # aggregate step metics\n",
        "        loss = torch.cat([x[\"loss\"].reshape(1) for x in outputs])\n",
        "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
        "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
        "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
        "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
        "\n",
        "        mean_loss = torch.mean(loss)\n",
        "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
        "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
        "        # in this particular case will not be much, however for dataset\n",
        "        # with \"empty\" images (images without target class) a large gap could be observed.\n",
        "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
        "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
        "\n",
        "        metrics = {\n",
        "            f\"{stage}_loss\": mean_loss,\n",
        "            f\"{stage}_dataset_iou\": dataset_iou\n",
        "        }\n",
        "\n",
        "        self.log_dict(metrics, prog_bar=True)\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "      self.train_outputs = []\n",
        "    def on_validation_epoch_start(self):\n",
        "      self.validation_outputs = []\n",
        "    def on_test_epoch_start(self):\n",
        "      self.test_outputs = []\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self.shared_step(batch, \"train\")\n",
        "        self.train_outputs.append(outputs)\n",
        "        return outputs['loss']\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self.shared_step(batch, \"valid\")\n",
        "        self.validation_outputs.append(outputs)\n",
        "        return outputs['loss']\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        outputs = self.shared_step(batch, \"test\")\n",
        "        self.test_outputs_outputs.append(outputs)\n",
        "        return outputs['loss']\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        return self.shared_epoch_end(self.train_outputs, \"train\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "\n",
        "        return self.shared_epoch_end(self.validation_outputs, \"valid\")\n",
        "    def on_test_epoch_end(self):\n",
        "        return self.shared_epoch_end(self.test_outputs, \"test\")\n",
        "\n",
        "    # def configure_optimizers(self):\n",
        "    #       optim = torch.optim.Adam(self.parameters(), lr=0.0001, weight_decay=1e-6) # Adding weight decay = 1e-5 to start\n",
        "\n",
        "    #       scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=1.0)\n",
        "    #       scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.8)\n",
        "\n",
        "    #       scheduler = torch.optim.lr_scheduler.SequentialLR(optim, [scheduler1, scheduler2], milestones=[15])\n",
        "    #       return [optim], [scheduler]\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "          optim = torch.optim.Adam(self.parameters(), lr=0.0001, weight_decay=2.0e-6)\n",
        "\n",
        "          return [optim], [torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0eef332",
      "metadata": {
        "id": "a0eef332"
      },
      "outputs": [],
      "source": [
        "model = ResModel(\"resnet34\", in_channels=6, out_classes=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MouqFb5O5hq4",
      "metadata": {
        "id": "MouqFb5O5hq4"
      },
      "outputs": [],
      "source": [
        "\n",
        "early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(monitor=\"valid_loss\", mode='min', min_delta=0.00, patience=10)\n",
        "\n",
        "# checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=\"/content/\", save_top_k=2, monitor=\"valid_dataset_iou\", mode='max')\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=\"/content/\", save_top_k=2, monitor=\"valid_loss\", mode='min')\n",
        "\n",
        "logger = pl.loggers.CSVLogger(\"/content/logs/\", name=\"manet_resnet_ls8_10band_model31_SR_v21\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00535da1",
      "metadata": {
        "id": "00535da1"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(\n",
        "    x_train_dir,\n",
        "    y_train_dir,\n",
        "    preprocessing=get_preprocessing(),\n",
        "    augmentation=get_training_augmentation(),\n",
        "    mean_std=mean_std,\n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir,\n",
        "    y_valid_dir,\n",
        "    preprocessing=get_preprocessing(),\n",
        "    mean_std=mean_std,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6668842",
      "metadata": {
        "id": "b6668842"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    accelerator='gpu',\n",
        "    max_epochs=80,\n",
        "    callbacks=[early_stop_callback, checkpoint_callback],\n",
        "    deterministic=True,\n",
        "    logger=logger\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=valid_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CP7_mDRK77qA",
      "metadata": {
        "id": "CP7_mDRK77qA"
      },
      "outputs": [],
      "source": [
        "!cp /content/*.ckpt /content/drive/MyDrive/pytorch_training/\n",
        "!cp -r /content/logs/* /content/drive/MyDrive/pytorch_training/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8nop1gzkUF",
      "metadata": {
        "id": "9c8nop1gzkUF"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nWjo_muBRhKr",
      "metadata": {
        "id": "nWjo_muBRhKr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}